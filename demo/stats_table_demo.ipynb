{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jrhosk/benchmarking.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'benchmarking'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6745fd6a6cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbenchmarking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstakeholder_tools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStatsTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'benchmarking'"
     ]
    }
   ],
   "source": [
    "from benchmarking.stakeholder_tools import StatsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StatsTable()` class provides access to interactive metrics and statistics information based on the returned stats dictionary returned by the `tclean` process.\n",
    "\n",
    "When instantiating `StatsTable()` the user must provide both the returned 'measured' dictionary and the 'expected' metrics dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = StatsTable(\n",
    "    json_expected='benchmarking/test_stk_alma_pipeline_imaging_exp_dicts.json', \n",
    "    json_measured='benchmarking/test_standard_cube_current_metrics_6.2.0.124.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stats_table()` method returns an interactive embedded table organized by tests that passed and test that failed. These are displayed in collasible tables and orgnaized by key. The user can reorganize each table by clicking at the top on the dataframe column that they want to organize by. The whole table is paginated for easier access and a pass/fail colums is included for further easy of reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.stats_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw JSON for each file can be accessed in an interactive panel using the `getter` methods, `StatsTable.json_measured()` and `StatsTable.json_expected()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.json_measured"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
